{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Experiments__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from collections import Counter\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '../data/clean_data_full.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num observations:\t52778\n"
     ]
    }
   ],
   "source": [
    "## Num rows\n",
    "print \"num observations:\\t\", len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unigram_phi(review):\n",
    "    return Counter(review.split())\n",
    "def bigram_phi(review):\n",
    "    return Counter(nltk.bigrams(review.split()))\n",
    "def unigram_bigram_phi(review):\n",
    "    return unigram_phi(review) + bigram_phi(review)\n",
    "def trigram_phi(review):\n",
    "    return Counter(nltk.trigrams(review.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Baseline:\n",
    "    def __init__(self):\n",
    "        self.avg_respsponse = 0\n",
    "    def fit(self, X, y):\n",
    "        self.avg_response = np.mean(y)\n",
    "    def predict(self, X):\n",
    "        rows, _ = X.shape\n",
    "        return np.repeat(self.avg_response, rows)\n",
    "\n",
    "def fit_baseline(X, y):\n",
    "    \"\"\"\n",
    "    Naive baseline return mean training prediction\n",
    "    \"\"\"\n",
    "    mod = Baseline()\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_linear_regression(X, y):    \n",
    "    \"\"\"\n",
    "    Linear Regression\n",
    "    \"\"\"\n",
    "    mod = LinearRegression(fit_intercept=True, n_jobs = -1)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_lasso(X, y, alpha = 0.1, max_iter = 1000):\n",
    "    lasso = Lasso(alpha = alpha, max_iter = max_iter)\n",
    "    mod = lasso.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_ridge(X, y, alpha = 0.1, max_iter = 1000):\n",
    "    lasso = Lasso(alpha = alpha, max_iter = max_iter)\n",
    "    mod = lasso.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_gbm_regression(X, y,\n",
    "                       n_estimators = 10,\n",
    "                       learning_rate = 0.1,\n",
    "                       max_depth = 1,\n",
    "                       random_state = 0,\n",
    "                       loss = \"ls\"):\n",
    "    \"\"\"\n",
    "    Gradient Boosting Method Regression\n",
    "    \"\"\"\n",
    "    gbm = GradientBoostingRegressor(n_estimators = n_estimators,\n",
    "                                    learning_rate = learning_rate,\n",
    "                                    max_depth = max_depth,\n",
    "                                    random_state = random_state,\n",
    "                                    loss = \"ls\")\n",
    "    mod = gbm.fit(X.toarray(), y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_rf_regression(X, y,\n",
    "                      n_estimators = 10,\n",
    "                      max_depth = None,\n",
    "                      max_features = 'auto',\n",
    "                      n_jobs = -1):\n",
    "    \"\"\"\n",
    "    Gradient Boosting Method Regression\n",
    "    \"\"\"\n",
    "    rf = RandomForestRegressor(n_estimators = n_estimators,\n",
    "                               max_depth = max_depth,\n",
    "                               max_features = max_features,\n",
    "                               n_jobs = n_jobs)\n",
    "    mod = rf.fit(X.toarray(), y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_adaBoost_regression(X, y,\n",
    "                            n_estimators = 50,\n",
    "                            learning_rate = 1,\n",
    "                            loss = \"linear\"):\n",
    "    \"\"\"\n",
    "    Gradient Boosting Method Regression\n",
    "    \"\"\"\n",
    "    adaBoost = AdaBoostRegressor(n_estimators = n_estimators,\n",
    "                           learning_rate = learning_rate,\n",
    "                           loss = loss)\n",
    "    mod = adaBoost.fit(X.toarray(), y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_data_set(data, vectorizer = None, aspect_str = \"OVERALL\"):\n",
    "    \"\"\"\n",
    "    Aspect ratings cols:\n",
    "        (7)  review_palate_score      -- score_normalizer = 1\n",
    "        (8)  review_taste_score       -- score_normalizer = 2\n",
    "        (11) review_aroma_score       -- score_normalizer = 2\n",
    "        (14) review_avg_score         -- score_normalizer = 1\n",
    "        (18) review_overall_score     -- score_normalizer = 4\n",
    "        (20) review_appearance_score  -- score_normalizer = 1\n",
    "        \n",
    "    predict_col :: column for aspect we're predicting, current default is column 8 (TASTE_SCORE)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ## RateBeer scrape data locations\n",
    "    ## ------------------------------\n",
    "    REVIEW_BLOB = 24\n",
    "    ASPECTS = {\n",
    "        \"PALATE\"     : [7, 1],\n",
    "        \"TASTE\"      : [8, 2],\n",
    "        \"AROMA\"      : [11, 2],\n",
    "        \"AVERAGE\"    : [14, 1],\n",
    "        \"OVERALL\"    : [18, 4],\n",
    "        \"APPEARANCE\" : [20, 1]\n",
    "    }\n",
    "    assert aspect_str in ASPECTS\n",
    "        \n",
    "    aspect_column = ASPECTS[aspect_str][0]      ## Get aspect rating column\n",
    "    aspect_normalizer = ASPECTS[aspect_str][1]  ## Get aspect normalizer\n",
    "    labels = []                                 ## Ratings\n",
    "    feat_dicts = []                             ## Features\n",
    "    raw_examples = []                           ## Review strings\n",
    "    data_values = data.values                   ## Data from pandas df\n",
    "    for row in data.values:\n",
    "        review, score = row[REVIEW_BLOB], row[aspect_column]\n",
    "        score = float(score) / aspect_normalizer\n",
    "\n",
    "        ## Safety check\n",
    "        if not isinstance(review, basestring):\n",
    "            print 'weird review:\\t', review\n",
    "            \n",
    "#         feat_dicts.append(phi(review))\n",
    "        labels.append(score)\n",
    "        raw_examples.append(review)\n",
    "        \n",
    "    # In training, we want a new vectorizer:\n",
    "    if vectorizer == None:\n",
    "        vectorizer = DictVectorizer(sparse=True)\n",
    "        feat_matrix = vectorizer.fit_transform(feat_dicts)\n",
    "    # In assessment, we featurize using the existing vectorizer:\n",
    "    else:\n",
    "        feat_matrix = vectorizer.fit_transform(raw_examples)\n",
    "\n",
    "    return {'X'            : feat_matrix, \n",
    "            'y'            : labels, \n",
    "            'vectorizer'   : vectorizer, \n",
    "            'raw_examples' : raw_examples,\n",
    "            'feature_names' : vectorizer.get_feature_names()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def experiment(data,\n",
    "               model = fit_lasso,\n",
    "               phi = None,\n",
    "               assess_data = None,\n",
    "               train_size = 0.9,\n",
    "               score_metrics = [mean_squared_error],\n",
    "               verbose = False):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    ## Build data set\n",
    "    # train = build_data_set(train_data, phi, vectorizer = vectorizer)\n",
    "    X_train = data['X'] \n",
    "    y_train = data['y']\n",
    "    vectorizer = data['vectorizer']\n",
    "    feature_names = data['feature_names']\n",
    "\n",
    "    ## Test-train split\n",
    "    if assess_data == None:\n",
    "        X_train, X_assess, y_train, y_assess = train_test_split(\n",
    "                X_train, y_train, train_size = train_size)\n",
    "    ## Only use for test-set\n",
    "    else:\n",
    "        assess = build_data_set(assess_data, phi, vectorizer = vectorizer)\n",
    "        X_assess, y_assess = assess['X'], assess['y']\n",
    "        \n",
    "\n",
    "    ## Model data\n",
    "    if model == None:\n",
    "        predictions = rep(np.mean(X_assess), len(y_assess))\n",
    "    mod = model(X_train, y_train)\n",
    "    predictions = mod.predict(X_assess.toarray())\n",
    "    \n",
    "    run_time = time.time() - start_time\n",
    "    if verbose:\n",
    "        print \"\\tExperiment information\"\n",
    "        print '\\t======================='\n",
    "        print \"\\ttrained:\\t\", model.__name__\n",
    "        print \"\\tnum training observations:\\t\", X_train.shape[0]\n",
    "        print \"\\tnum training features:\\t\", X_train.shape[1]\n",
    "        print \"\\tscore_metrics:\\t\", [score_metric.__name__ for score_metric in\\\n",
    "                                     score_metrics]\n",
    "        print \"\\trun time: \", run_time\n",
    "        if model.__name__ == 'fit_linear_regression' or\\\n",
    "            model.__name__ == 'fit_lasso' or\\\n",
    "            model.__name__ == 'fit_ridge':\n",
    "            print feature_names[1:10]\n",
    "            print mod.coef_[1]\n",
    "        print \"y_assess[1:10]:\\t\", y_assess[1:10]\n",
    "        print \"predictions[1:10]:\\t\", predictions[1:10]\n",
    "    \n",
    "    ## Return MSE\n",
    "    return [score_metric(y_assess, predictions) for score_metric in score_metrics]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_reader(train_path = data_path, n = 1000):\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df.sample(n)\n",
    "\n",
    "def dev_reader(test_path, n = 100):\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df.sample(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer's encode phi..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## bigram_vectorizer\n",
    "## -----------------\n",
    "## normalization:\n",
    "##  * lower\n",
    "##  * remove stop words\n",
    "##  * min word length == 2\n",
    "##\n",
    "bigram_vectorizer = CountVectorizer(analyzer='word', stop_words = 'english', ngram_range=(2, 2), min_df = 3, max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## unigram_vectorizer\n",
    "## -----------------\n",
    "## normalization:\n",
    "##  * lower\n",
    "##  * remove stop words\n",
    "##  * min word length == 2\n",
    "##\n",
    "unigram_vectorizer = CountVectorizer(analyzer='word', stop_words = 'english', ngram_range=(1, 1), min_df = 3, max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## trigram_vectorizer\n",
    "## ------------------\n",
    "## normalization:\n",
    "##  * lower\n",
    "##  * remove stop words\n",
    "##  * min word length == 2\n",
    "##\n",
    "trigram_vectorizer = CountVectorizer(analyzer='word', stop_words = 'english', ngram_range=(3, 3), min_df = 3, max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_vectorizer(n_gram = \"unigram\", min_df = 3, sample_size = 10000, stop_words = None, features_prop = 0.25):\n",
    "\n",
    "    max_features = int(sample_size * features_prop)\n",
    "    if n_gram == \"trigram\":\n",
    "        ngram_range = (3,3)\n",
    "    elif n_gram == \"bigram\":\n",
    "        ngram_range = (2,2)\n",
    "    else: # default to unigram\n",
    "        ngram_range = (1,1)\n",
    "    \n",
    "    return CountVectorizer(analyzer='word',\n",
    "                           ngram_range = ngram_range,\n",
    "                           min_df = min_df,\n",
    "                           max_features = max_features,\n",
    "                           stop_words = stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def build_data_set(data, phi, vectorizer = None, aspect_str = \"OVERALL\"):\n",
    "\n",
    "n_samples = 2000\n",
    "vectorizer = set_vectorizer(n_gram = \"unigram\", sample_size = n_samples)\n",
    "train_d = train_reader(data_path, n = n_samples)\n",
    "built_data_set = build_data_set(train_d, vectorizer = vectorizer, aspect_str = 'TASTE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelim runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExperiment information\n",
      "\t=======================\n",
      "\ttrained:\tfit_baseline\n",
      "\tnum training observations:\t1800\n",
      "\tnum training features:\t500\n",
      "\tscore_metrics:\t['mean_squared_error', 'r2_score']\n",
      "\trun time:  0.00462913513184\n",
      "y_assess[1:10]:\t[3.0, 4.5, 4.0, 3.0, 5.0, 4.0, 3.5, 0.5, 3.0]\n",
      "predictions[1:10]:\t[ 3.43722222  3.43722222  3.43722222  3.43722222  3.43722222  3.43722222\n",
      "  3.43722222  3.43722222  3.43722222]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.152904938271605, -0.011547497647260707]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment(data = built_data_set,\n",
    "           model = fit_baseline,\n",
    "           verbose = True,\n",
    "           score_metrics = [mean_squared_error, r2_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExperiment information\n",
      "\t=======================\n",
      "\ttrained:\tfit_linear_regression\n",
      "\tnum training observations:\t36000\n",
      "\tnum training features:\t15501\n",
      "\tscore_metrics:\t['mean_squared_error', 'r2_score']\n",
      "\trun time:  79.4378790855\n",
      "[u'000', u'001', u'002', u'00euro', u'01', u'02', u'03', u'04', u'05']\n",
      "-0.120951848559\n",
      "y_assess[1:10]:\t[4.5, 4.5, 2.0, 4.0, 2.5, 3.0, 3.5, 3.5, 1.0]\n",
      "predictions[1:10]:\t[ 3.28589436  4.38568174  3.08920484  3.83228108  2.39578727  2.2655702\n",
      "  3.58327387  3.87744199  1.61476294]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.8100279477403411, -9.4117145892123464]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Note: running time ~ 1 minute @ 40k observations\n",
    "experiment(data = built_data_set,\n",
    "           model = fit_linear_regression,\n",
    "           verbose = True,\n",
    "           score_metrics = [mean_squared_error, r2_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExperiment information\n",
      "\t=======================\n",
      "\ttrained:\tfit_ridge\n",
      "\tnum training observations:\t1800\n",
      "\tnum training features:\t500\n",
      "\tscore_metrics:\t['mean_squared_error', 'r2_score']\n",
      "\trun time:  0.00813412666321\n",
      "[u'12', u'2016', u'about', u'absolutely', u'abv', u'actually', u'after', u'aftertaste', u'again']\n",
      "0.0\n",
      "y_assess[1:10]:\t[4.0, 3.0, 3.5, 4.0, 5.0, 2.0, 4.0, 3.0, 5.0]\n",
      "predictions[1:10]:\t[ 3.47979801  3.41891308  3.38847062  3.44935554  3.38847062  3.38847062\n",
      "  3.41891308  3.47979801  3.38847062]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.79563726261415979, 0.01070134195738548]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Note: running time few seconds @ 40k observations\n",
    "experiment(data = built_data_set,\n",
    "           model = fit_ridge,\n",
    "           verbose = True, \n",
    "           score_metrics = [mean_squared_error, r2_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExperiment information\n",
      "\t=======================\n",
      "\ttrained:\tfit_lasso\n",
      "\tnum training observations:\t1800\n",
      "\tnum training features:\t500\n",
      "\tscore_metrics:\t['mean_squared_error', 'r2_score']\n",
      "\trun time:  0.00907516479492\n",
      "[u'12', u'2016', u'about', u'absolutely', u'abv', u'actually', u'after', u'aftertaste', u'again']\n",
      "0.0\n",
      "y_assess[1:10]:\t[3.0, 2.5, 3.5, 3.0, 3.5, 3.5, 3.5, 3.5, 2.5]\n",
      "predictions[1:10]:\t[ 3.48098545  3.54827392  3.44734121  3.38005275  3.44734121  3.38005275\n",
      "  3.41369698  3.41369698  3.41369698]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.85232723572085889, 0.0058512286106257028]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Note: running time few seconds @ 40k observations\n",
    "experiment(data = built_data_set,\n",
    "           model = fit_lasso,\n",
    "           verbose = True,\n",
    "           score_metrics = [mean_squared_error, r2_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExperiment information\n",
      "\t=======================\n",
      "\ttrained:\tfit_rf_regression\n",
      "\tnum training observations:\t1800\n",
      "\tnum training features:\t500\n",
      "\tscore_metrics:\t['mean_squared_error', 'r2_score']\n",
      "\trun time:  0.987848043442\n",
      "y_assess[1:10]:\t[4.5, 3.5, 4.0, 3.5, 4.5, 4.0, 3.5, 4.5, 4.0]\n",
      "predictions[1:10]:\t[ 3.6         3.65        3.75        3.28928481  3.95        3.2         3.85\n",
      "  3.2         3.6       ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.88655438772582673, 0.14424735889977458]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Note: run time ~3min with 10K obs\n",
    "experiment(data = built_data_set,\n",
    "           model = fit_rf_regression,\n",
    "           verbose = True,\n",
    "           score_metrics = [mean_squared_error, r2_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExperiment information\n",
      "\t=======================\n",
      "\ttrained:\tfit_gbm_regression\n",
      "\tnum training observations:\t1800\n",
      "\tnum training features:\t500\n",
      "\tscore_metrics:\t['mean_squared_error', 'r2_score']\n",
      "\trun time:  1.43827581406\n",
      "y_assess[1:10]:\t[4.5, 5.0, 3.0, 4.0, 3.5, 4.0, 4.0, 3.5, 4.0]\n",
      "predictions[1:10]:\t[ 3.50786881  3.11274859  3.34864512  3.71601122  3.22677156  3.27947452\n",
      "  3.60843426  3.31110121  3.27947452]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.74883944170835059, 0.10987555590223097]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: running time is currently ver long\n",
    "experiment(data = built_data_set,\n",
    "           model = fit_gbm_regression,\n",
    "           verbose = True,\n",
    "           score_metrics = [mean_squared_error, r2_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExperiment information\n",
      "\t=======================\n",
      "\ttrained:\tfit_adaBoost_regression\n",
      "\tnum training observations:\t1800\n",
      "\tnum training features:\t500\n",
      "\tscore_metrics:\t['mean_squared_error', 'r2_score']\n",
      "\trun time:  0.435350894928\n",
      "y_assess[1:10]:\t[4.0, 3.5, 4.0, 3.5, 3.0, 4.0, 3.0, 3.5, 3.0]\n",
      "predictions[1:10]:\t[ 3.33852281  3.3041958   2.99123832  3.3041958   3.26788124  3.26788124\n",
      "  3.33852281  3.26993865  3.3041958 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.88254294320568105, 0.046517995672341161]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: running time is currently ver long\n",
    "experiment(data = built_data_set,\n",
    "           model = fit_adaBoost_regression,\n",
    "           verbose = True,\n",
    "           score_metrics = [mean_squared_error, r2_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Between aspects - lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------\n",
      "ngram:\tbigram\n",
      "curr aspect:\tOVERALL\n",
      "\tExperiment information\n",
      "\t=======================\n",
      "\ttrained:\tfit_lasso\n",
      "\tnum training observations:\t28000\n",
      "\tnum training features:\t20000\n",
      "\tscore_metric:\tmean_squared_error\n",
      "\trun time:  3.36163902283\n",
      "[u'03 07', u'04 05', u'04 07', u'04 2016', u'05 pours', u'06 07', u'06 08', u'07 07', u'07 poured']\n",
      "-0.0\n",
      "mse:\t0.959995435162\n",
      "\n",
      "-----------------\n",
      "ngram:\tbigram\n",
      "curr aspect:\tTASTE\n",
      "\tExperiment information\n",
      "\t=======================\n",
      "\ttrained:\tfit_lasso\n",
      "\tnum training observations:\t28000\n",
      "\tnum training features:\t20000\n",
      "\tscore_metric:\tmean_squared_error\n",
      "\trun time:  3.04476690292\n",
      "[u'04 07', u'04 2016', u'05 pours', u'06 07', u'06 08', u'07 07', u'07 poured', u'10 2012', u'10 2014']\n",
      "-0.0\n",
      "mse:\t0.910395687819\n",
      "\n",
      "-----------------\n",
      "ngram:\tbigram\n",
      "curr aspect:\tAROMA\n",
      "\tExperiment information\n",
      "\t=======================\n",
      "\ttrained:\tfit_lasso\n",
      "\tnum training observations:\t28000\n",
      "\tnum training features:\t20000\n",
      "\tscore_metric:\tmean_squared_error\n",
      "\trun time:  3.11804795265\n",
      "[u'04 07', u'04 2016', u'05 pours', u'06 07', u'06 08', u'07 07', u'07 poured', u'10 2012', u'10 2014']\n",
      "-0.0\n",
      "mse:\t0.947110688776\n",
      "\n",
      "-----------------\n",
      "ngram:\tbigram\n",
      "curr aspect:\tPALATE\n",
      "\tExperiment information\n",
      "\t=======================\n",
      "\ttrained:\tfit_lasso\n",
      "\tnum training observations:\t28000\n",
      "\tnum training features:\t20000\n",
      "\tscore_metric:\tmean_squared_error\n",
      "\trun time:  3.31642413139\n",
      "[u'04 07', u'04 2016', u'05 pours', u'06 07', u'06 08', u'07 07', u'07 poured', u'10 10', u'10 2012']\n",
      "-0.0\n",
      "mse:\t0.881530042942\n",
      "\n",
      "-----------------\n",
      "ngram:\tbigram\n",
      "curr aspect:\tAPPEARANCE\n",
      "\tExperiment information\n",
      "\t=======================\n",
      "\ttrained:\tfit_lasso\n",
      "\tnum training observations:\t28000\n",
      "\tnum training features:\t20000\n",
      "\tscore_metric:\tmean_squared_error\n",
      "\trun time:  3.07668495178\n",
      "[u'04 07', u'04 2016', u'05 pours', u'06 07', u'06 08', u'07 07', u'07 poured', u'10 2012', u'10 abv']\n",
      "-0.0\n",
      "mse:\t0.807532514031\n"
     ]
    }
   ],
   "source": [
    "ngram = 'bigram'\n",
    "ASPECTS = ['OVERALL', 'TASTE', 'AROMA', 'PALATE', 'APPEARANCE']\n",
    "for aspect in ASPECTS:\n",
    "    n_samples = 40000\n",
    "    vectorizer = set_vectorizer(n_gram = ngram, sample_size = n_samples)\n",
    "    train_d = train_reader(data_path, n = n_samples)\n",
    "    built_data_set = build_data_set(train_d, vectorizer = vectorizer, aspect_str = aspect)\n",
    "\n",
    "    print \"\\n-----------------\"\n",
    "    print \"ngram:\\t\", ngram\n",
    "    print \"curr aspect:\\t\", aspect\n",
    "    mse = experiment(data = built_data_set,\n",
    "           model = fit_lasso,\n",
    "           verbose = True)\n",
    "    print \"mse:\\t\", mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learners = [fit_linear_regression, fit_lasso, fit_gbm_regression]\n",
    "vectorizers = [unigram_vectorizer, bigram_vectorizer]\n",
    "vectorizer_names = [\"unigram\", \"bigram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "Fitting:\tfit_linear_regression\n",
      "-------\n",
      "vectorizer:\tunigram\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-5bf7d8440306>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                          \u001b[0munigram_phi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                          \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                          verbose = True)\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"mse:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-4f8ebdd3f064>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(train_data, model, phi, assess_data, train_size, score_metric, vectorizer, verbose)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m## Build data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_data_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-98a8b234f812>\u001b[0m in \u001b[0;36mbuild_data_set\u001b[0;34m(data, phi, vectorizer)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# In assessment, we featurize using the existing vectorizer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mfeat_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     return {'X': feat_matrix, \n",
      "\u001b[0;32m/Users/benpeloquin/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 804\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/benpeloquin/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/benpeloquin/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'char_wb'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             return lambda doc: self._char_wb_ngrams(\n\u001b[0;32m--> 229\u001b[0;31m                 preprocess(self.decode(doc)))\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'word'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/benpeloquin/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_char_wb_ngrams\u001b[0;34m(self, text_document)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mw_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0moffset\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mngrams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# count a short word (w_len < n) only once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for learner in learners:\n",
    "    print \"\\n=============================\"\n",
    "    print \"Fitting:\\t\", learner.__name__\n",
    "    for vectorizer, name in zip(vectorizers, vectorizer_names):\n",
    "        print \"-------\"\n",
    "        print \"vectorizer:\\t\", name\n",
    "        mse = experiment(train_d,\n",
    "                         learner,\n",
    "                         unigram_phi,\n",
    "                         vectorizer = vectorizer,\n",
    "                         verbose = True)\n",
    "        print \"mse:\\t\", mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_model(d_sample, model, phi, n_samples = 100):\n",
    "    res = [experiment(data, model, phi) for _ in range(n_samples)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glm_res = run_model(train, fit_linear_regression, trigram_phi)\n",
    "gbm_res = run_model(train, fit_gbm_regression, trigram_phi)\n",
    "lasso_res = run_model(train, fit_lasso, trigram_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm: 5.09104430619 1.65291139452\n",
      "gbm: 5.4193393923 2.1477179012\n",
      "lasso 5.5710284521 1.65857267327\n"
     ]
    }
   ],
   "source": [
    "print \"glm:\", np.mean(glm_res), np.var(glm_res)\n",
    "print \"gbm:\", np.mean(gbm_res), np.var(gbm_res)\n",
    "print \"lasso\", np.mean(lasso_res), np.var(lasso_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0933661297989\n",
      "0.0212407456292\n",
      "0.5025560931\n"
     ]
    }
   ],
   "source": [
    "print scipy.stats.wilcoxon(glm_res, gbm_res)[1]\n",
    "print scipy.stats.wilcoxon(glm_res, lasso_res)[1]\n",
    "print scipy.stats.wilcoxon(gbm_res, lasso_res)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
